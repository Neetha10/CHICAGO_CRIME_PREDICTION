{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f286e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde6fc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/homebrew/lib/python3.14/site-packages (4.0.1)\n",
      "Requirement already satisfied: pyarrow in /opt/homebrew/lib/python3.14/site-packages (22.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /opt/homebrew/lib/python3.14/site-packages (from pyspark) (0.10.9.9)\n",
      "env: JAVA_HOME=/opt/homebrew/opt/openjdk@17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/29 22:18:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/29 22:18:16 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp dir: /Users/kiranghumare/spark_tmp\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark pyarrow --break-system-packages\n",
    "%env JAVA_HOME=/opt/homebrew/opt/openjdk@17\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"crime_hotspot\")\n",
    "    .config(\"spark.local.dir\", \"/Users/kiranghumare/spark_tmp\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")        # increase driver memory\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")  # bind to localhost\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")         # force driver host to localhost\n",
    "    .config(\"spark.blockManager.port\", \"0\") \n",
    "    .config(\"spark.ui.port\", \"4060\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Temp dir:\", spark.conf.get(\"spark.local.dir\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb698fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV exists: True\n",
      "Parquet out: /Users/kiranghumare/Desktop/CHICAGO_CRIME_PREDICTION/outputs/clean_crimes_parquet\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "\n",
    "# BASE_DIR = os.getenv(\n",
    "#     \"CRIME_PROJECT_BASE\",\n",
    "#     os.path.abspath(os.path.join(os.getcwd(), \"CHICAGO_CRIME_PREDICTION\"))\n",
    "# )\n",
    "\n",
    "# print(\"BASE_DIR: \", BASE_DIR)\n",
    "# DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")\n",
    "# OUTPUT_DIR = os.path.join(BASE_DIR, \"outputs\", \"clean_crimes_parquet\")\n",
    "\n",
    "# CRIMES_CSV = os.path.join(DATA_DIR, \"chicago_crimes_2001_present.csv\")\n",
    "\n",
    "# os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# print(\"Base directory:\", BASE_DIR)\n",
    "# print(\"CSV exists:\", os.path.exists(CRIMES_CSV))\n",
    "# print(\"Parquet output directory:\", os.path.abspath(OUTPUT_DIR))\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# ✅ Paths customized for your project\n",
    "CRIMES_CSV  = Path(\"/Users/kiranghumare/Desktop/CHICAGO_CRIME_PREDICTION/data/raw/chicago_crimes_2001_present.csv\")\n",
    "OUT_PARQUET = Path(\"/Users/kiranghumare/Desktop/CHICAGO_CRIME_PREDICTION/outputs/clean_crimes_parquet\")\n",
    "\n",
    "OUT_PARQUET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CSV exists:\", CRIMES_CSV.exists())\n",
    "print(\"Parquet out:\", OUT_PARQUET.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af912a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw row count: 8443668\n",
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: boolean (nullable = true)\n",
      " |-- Domestic: boolean (nullable = true)\n",
      " |-- Beat: integer (nullable = true)\n",
      " |-- District: integer (nullable = true)\n",
      " |-- Ward: integer (nullable = true)\n",
      " |-- Community Area: integer (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: integer (nullable = true)\n",
      " |-- Y Coordinate: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n",
      "+--------+-----------+----------------------+----------------------+----+-----------------------+------------------------------------------------------------+---------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n",
      "|ID      |Case Number|Date                  |Block                 |IUCR|Primary Type           |Description                                                 |Location Description       |Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|Updated On            |Latitude    |Longitude    |Location                     |\n",
      "+--------+-----------+----------------------+----------------------+----+-----------------------+------------------------------------------------------------+---------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n",
      "|14027309|JJ487531   |11/09/2025 12:00:00 AM|019XX E 71ST PL       |2825|OTHER OFFENSE          |HARASSMENT BY TELEPHONE                                     |APARTMENT                  |false |false   |333 |3       |5   |43            |26      |1190562     |1857968     |2025|11/16/2025 03:41:53 PM|41.765286035|-87.577086035|(41.765286035, -87.577086035)|\n",
      "|14023982|JJ483314   |11/09/2025 12:00:00 AM|008XX W WELLINGTON AVE|0281|CRIMINAL SEXUAL ASSAULT|NON-AGGRAVATED                                              |HOSPITAL BUILDING / GROUNDS|false |false   |1933|19      |44  |6             |02      |1169981     |1920130     |2025|11/16/2025 03:41:53 PM|41.936335635|-87.650710119|(41.936335635, -87.650710119)|\n",
      "|14024436|JJ483983   |11/09/2025 12:00:00 AM|025XX W POTOMAC AVE   |1156|DECEPTIVE PRACTICE     |ATTEMPT - FINANCIAL IDENTITY THEFT                          |APARTMENT                  |false |false   |1423|14      |26  |24            |11      |1159396     |1908562     |2025|11/16/2025 03:41:53 PM|41.904816889|-87.689930043|(41.904816889, -87.689930043)|\n",
      "|14024315|JJ483686   |11/09/2025 12:00:00 AM|016XX W JACKSON BLVD  |0281|CRIMINAL SEXUAL ASSAULT|NON-AGGRAVATED                                              |HOTEL / MOTEL              |false |false   |1231|12      |34  |28            |02      |1165553     |1898694     |2025|11/16/2025 03:41:53 PM|41.877609409|-87.667594837|(41.877609409, -87.667594837)|\n",
      "|14027095|JJ487199   |11/09/2025 12:00:00 AM|032XX W 15TH ST       |1577|SEX OFFENSE            |CRIM SEX ABUSE-PENETRATE - OFF < 5 YRS OLDER - VIC 13-16 YOA|APARTMENT                  |false |true    |1022|10      |24  |29            |17      |1154856     |1892525     |2025|11/16/2025 03:41:53 PM|41.860901987|-87.707036785|(41.860901987, -87.707036785)|\n",
      "+--------+-----------+----------------------+----------------------+----+-----------------------+------------------------------------------------------------+---------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Read the full dataset into Spark\n",
    "crimes_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(str(CRIMES_CSV))\n",
    ")\n",
    "\n",
    "print(\"Raw row count:\", crimes_raw.count())\n",
    "crimes_raw.printSchema()\n",
    "crimes_raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb027a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------+\n",
      "|date                  |date_ts            |\n",
      "+----------------------+-------------------+\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "|11/09/2025 12:00:00 AM|2025-11-09 00:00:00|\n",
      "+----------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "def to_snake(name: str) -> str:\n",
    "    return (\n",
    "        name.lower()\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"/\", \"_\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )\n",
    "\n",
    "df = crimes_raw\n",
    "for c in df.columns:\n",
    "    df = df.withColumnRenamed(c, to_snake(c))\n",
    "\n",
    "# Convert date to timestamp (correct Chicago format)\n",
    "df = df.withColumn(\"date_ts\", F.to_timestamp(\"date\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Quick sanity check\n",
    "df.select(\"date\", \"date_ts\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff242bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================================>           (20 + 5) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned row count: 8349460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Drop duplicates\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Remove rows with nulls in key fields\n",
    "critical_cols = [\"primary_type\", \"date_ts\", \"latitude\", \"longitude\"]\n",
    "for col in critical_cols:\n",
    "    df = df.filter(F.col(col).isNotNull())\n",
    "\n",
    "# Chicago bounding box filter\n",
    "df = df.filter(\n",
    "    (F.col(\"latitude\").between(41.60, 42.10)) &\n",
    "    (F.col(\"longitude\").between(-87.95, -87.50))\n",
    ")\n",
    "\n",
    "# Remove future-dated records\n",
    "df = df.filter(F.col(\"date_ts\") <= F.current_timestamp())\n",
    "\n",
    "print(\"Cleaned row count:\", df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c88a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----+----+---------+------+---------+---------+\n",
      "|date_ts            |year|month|hour|dayofweek|season|lat_round|lon_round|\n",
      "+-------------------+----+-----+----+---------+------+---------+---------+\n",
      "|2025-11-08 21:27:00|2025|11   |21  |Sat      |fall  |41.806   |-87.624  |\n",
      "|2025-11-08 14:59:00|2025|11   |14  |Sat      |fall  |41.932   |-87.712  |\n",
      "|2025-11-08 14:30:00|2025|11   |14  |Sat      |fall  |41.882   |-87.624  |\n",
      "|2025-11-08 13:45:00|2025|11   |13  |Sat      |fall  |41.75    |-87.674  |\n",
      "|2025-11-08 12:29:00|2025|11   |12  |Sat      |fall  |41.883   |-87.628  |\n",
      "+-------------------+----+-----+----+---------+------+---------+---------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"year\", F.year(\"date_ts\"))\n",
    "    .withColumn(\"month\", F.month(\"date_ts\"))\n",
    "    .withColumn(\"hour\", F.hour(\"date_ts\"))\n",
    "    .withColumn(\"dayofweek\", F.date_format(\"date_ts\", \"E\"))\n",
    "    .withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.col(\"month\").isin(12, 1, 2), \"winter\")\n",
    "         .when(F.col(\"month\").isin(3, 4, 5), \"spring\")\n",
    "         .when(F.col(\"month\").isin(6, 7, 8), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    )\n",
    "    .withColumn(\"lat_round\", F.round(\"latitude\", 3))\n",
    "    .withColumn(\"lon_round\", F.round(\"longitude\", 3))\n",
    ")\n",
    "\n",
    "df.select(\"date_ts\",\"year\",\"month\",\"hour\",\"dayofweek\",\"season\",\"lat_round\",\"lon_round\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a60ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 22:18:44 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+------+--------------+--------+------------+------------+----+----------+--------+---------+--------+-------+-----+----+---------+------+---------+---------+\n",
      "| id|case_number|date|block|iucr|primary_type|description|location_description|arrest|domestic|beat|district|  ward|community_area|fbi_code|x_coordinate|y_coordinate|year|updated_on|latitude|longitude|location|date_ts|month|hour|dayofweek|season|lat_round|lon_round|\n",
      "+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+------+--------------+--------+------------+------------+----+----------+--------+---------+--------+-------+-----+----+---------+------+---------+---------+\n",
      "|  0|          0|   0|    0|   0|           0|          0|                9856|     0|       0|   0|      47|605530|        604443|       0|           0|           0|   0|         0|       0|        0|       0|      0|    0|   0|        0|     0|        0|        0|\n",
      "+---+-----------+----+-----+----+------------+-----------+--------------------+------+--------+----+--------+------+--------------+--------+------------+------------+----+----------+--------+---------+--------+-------+-----+----+---------+------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 8349460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:============================================>          (40 + 10) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct records: 8349460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"Total records: {df.count()}\")\n",
    "print(f\"Distinct records: {df.distinct().count()}\")\n",
    "\n",
    "# Handle missing values in important columns\n",
    "df = df.filter(F.col(\"latitude\").isNotNull() & F.col(\"longitude\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c1a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|       primary_type|  count|\n",
      "+-------------------+-------+\n",
      "|              THEFT|1771345|\n",
      "|            BATTERY|1531729|\n",
      "|    CRIMINAL DAMAGE| 955513|\n",
      "|          NARCOTICS| 751405|\n",
      "|            ASSAULT| 563665|\n",
      "|      OTHER OFFENSE| 521951|\n",
      "|           BURGLARY| 444542|\n",
      "|MOTOR VEHICLE THEFT| 428752|\n",
      "| DECEPTIVE PRACTICE| 367410|\n",
      "|            ROBBERY| 313399|\n",
      "+-------------------+-------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|hour| count|\n",
      "+----+------+\n",
      "|   0|474376|\n",
      "|   1|267066|\n",
      "|   2|225999|\n",
      "|   3|184571|\n",
      "|   4|140933|\n",
      "|   5|118202|\n",
      "|   6|136053|\n",
      "|   7|192788|\n",
      "|   8|281913|\n",
      "|   9|355841|\n",
      "|  10|353185|\n",
      "|  11|369807|\n",
      "|  12|475985|\n",
      "|  13|395048|\n",
      "|  14|419606|\n",
      "|  15|445681|\n",
      "|  16|424446|\n",
      "|  17|431707|\n",
      "|  18|456722|\n",
      "|  19|469013|\n",
      "+----+------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|season|  count|\n",
      "+------+-------+\n",
      "|spring|2108082|\n",
      "|  fall|2107607|\n",
      "|winter|1839713|\n",
      "|summer|2294058|\n",
      "+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:===============================================>        (21 + 4) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dayofweek|  count|\n",
      "+---------+-------+\n",
      "|      Fri|1252675|\n",
      "|      Tue|1189938|\n",
      "|      Mon|1181335|\n",
      "|      Sat|1198809|\n",
      "|      Wed|1198411|\n",
      "|      Thu|1186784|\n",
      "|      Sun|1141508|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Crime counts by type\n",
    "df.groupBy(\"primary_type\").count().orderBy(F.desc(\"count\")).show(10)\n",
    "\n",
    "# Crimes by hour\n",
    "df.groupBy(\"hour\").count().orderBy(\"hour\").show()\n",
    "\n",
    "# Crimes by season\n",
    "df.groupBy(\"season\").count().show()\n",
    "\n",
    "# Crimes by day of week\n",
    "df.groupBy(\"dayofweek\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d081c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c0e05ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MISSING VALUES CHECK\n",
      "======================================================================\n",
      "\n",
      "Missing values in NUMERIC columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+------+--------------+------------+------------+----+--------+---------+-----+----+---------+---------+\n",
      "| id|beat|district|  ward|community_area|x_coordinate|y_coordinate|year|latitude|longitude|month|hour|lat_round|lon_round|\n",
      "+---+----+--------+------+--------------+------------+------------+----+--------+---------+-----+----+---------+---------+\n",
      "|  0|   0|      47|605530|        604443|           0|           0|   0|       0|        0|    0|   0|        0|        0|\n",
      "+---+----+--------+------+--------------+------------+------------+----+--------+---------+-----+----+---------+---------+\n",
      "\n",
      "\n",
      "Missing values in NON-NUMERIC columns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:============================================>           (20 + 5) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+----+------------+-----------+--------------------+------+--------+--------+----------+--------+-------+---------+------+\n",
      "|case_number|date|block|iucr|primary_type|description|location_description|arrest|domestic|fbi_code|updated_on|location|date_ts|dayofweek|season|\n",
      "+-----------+----+-----+----+------------+-----------+--------------------+------+--------+--------+----------+--------+-------+---------+------+\n",
      "|          0|   0|    0|   0|           0|          0|                9856|     0|       0|       0|         0|       0|      0|        0|     0|\n",
      "+-----------+----+-----+----+------------+-----------+--------------------+------+--------+--------+----------+--------+-------+---------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check for missing values (handles both numeric and string columns)\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get numeric and non-numeric columns\n",
    "numeric_cols = [c for c, dtype in df.dtypes if dtype in ['double', 'float', 'int', 'bigint']]\n",
    "non_numeric_cols = [c for c in df.columns if c not in numeric_cols]\n",
    "\n",
    "# Check missing values for numeric columns (use isnan)\n",
    "if numeric_cols:\n",
    "    missing_numeric = df.select([\n",
    "        count(when(col(c).isNull() | isnan(c), c)).alias(c) \n",
    "        for c in numeric_cols\n",
    "    ])\n",
    "    print(\"\\nMissing values in NUMERIC columns:\")\n",
    "    missing_numeric.show()\n",
    "\n",
    "# Check missing values for non-numeric columns (only use isNull)\n",
    "if non_numeric_cols:\n",
    "    missing_non_numeric = df.select([\n",
    "        count(when(col(c).isNull(), c)).alias(c) \n",
    "        for c in non_numeric_cols\n",
    "    ])\n",
    "    print(\"\\nMissing values in NON-NUMERIC columns:\")\n",
    "    missing_non_numeric.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "615ffeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "======================================================================\n",
      "DOWNLOADING CHICAGO WEATHER DATA\n",
      "======================================================================\n",
      "\n",
      "1. Fetching weather data from Open-Meteo API...\n",
      "   (This may take a few minutes...)\n",
      "   ✓ Downloaded 8,766 days of weather data\n",
      "   ✓ Saved to: ../data/raw/chicago_weather_2001_2024.csv\n",
      "   ✓ Date range: 2001-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "\n",
      "   Sample weather data:\n",
      "        date  temp_max  temp_min  temp_mean  precipitation  wind_speed\n",
      "0 2001-01-01      22.2      12.4       17.1          0.000        11.5\n",
      "1 2001-01-02      15.1       4.2        9.8          0.000        15.3\n",
      "2 2001-01-03      26.4       9.6       19.2          0.008        16.9\n",
      "3 2001-01-04      30.9      16.5       23.6          0.000        22.3\n",
      "4 2001-01-05      34.0      25.0       31.1          0.004        17.5\n"
     ]
    }
   ],
   "source": [
    "# Download Chicago weather data\n",
    "%pip install requests\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DOWNLOADING CHICAGO WEATHER DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Option 1: Download from NOAA Climate Data Online\n",
    "# We'll use a pre-processed dataset from a reliable source\n",
    "\n",
    "weather_url = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access/\"\n",
    "\n",
    "# For simplicity, let's use a different approach - download historical weather from Open-Meteo\n",
    "# This is a free weather API with historical data\n",
    "\n",
    "print(\"\\n1. Fetching weather data from Open-Meteo API...\")\n",
    "print(\"   (This may take a few minutes...)\")\n",
    "\n",
    "# We'll get daily weather data for Chicago (2001-2024)\n",
    "# Chicago coordinates: 41.8781, -87.6298\n",
    "\n",
    "base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2001-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "params = {\n",
    "    \"latitude\": 41.8781,\n",
    "    \"longitude\": -87.6298,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"daily\": \"temperature_2m_max,temperature_2m_min,temperature_2m_mean,precipitation_sum,windspeed_10m_max\",\n",
    "    \"temperature_unit\": \"fahrenheit\",\n",
    "    \"windspeed_unit\": \"mph\",\n",
    "    \"precipitation_unit\": \"inch\",\n",
    "    \"timezone\": \"America/Chicago\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(base_url, params=params, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    weather_data = response.json()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    weather_df = pd.DataFrame({\n",
    "        'date': weather_data['daily']['time'],\n",
    "        'temp_max': weather_data['daily']['temperature_2m_max'],\n",
    "        'temp_min': weather_data['daily']['temperature_2m_min'],\n",
    "        'temp_mean': weather_data['daily']['temperature_2m_mean'],\n",
    "        'precipitation': weather_data['daily']['precipitation_sum'],\n",
    "        'wind_speed': weather_data['daily']['windspeed_10m_max']\n",
    "    })\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    weather_path = '../data/raw/chicago_weather_2001_2024.csv'\n",
    "    weather_df.to_csv(weather_path, index=False)\n",
    "    \n",
    "    print(f\"   ✓ Downloaded {len(weather_df):,} days of weather data\")\n",
    "    print(f\"   ✓ Saved to: {weather_path}\")\n",
    "    print(f\"   ✓ Date range: {weather_df['date'].min()} to {weather_df['date'].max()}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n   Sample weather data:\")\n",
    "    print(weather_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Error downloading weather data: {e}\")\n",
    "    print(\"   → We'll create a backup method...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d95eac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DOWNLOADING CHICAGO CENSUS/DEMOGRAPHIC DATA\n",
      "======================================================================\n",
      "\n",
      "2. Fetching census data from Chicago Data Portal...\n",
      "   ✓ Downloaded census data for 78 community areas\n",
      "   ✓ Saved to: ../data/raw/chicago_census_community_areas.csv\n",
      "\n",
      "   Sample census data:\n",
      "  ca community_area_name percent_of_housing_crowded  \\\n",
      "0  1         Rogers Park                        7.7   \n",
      "1  2          West Ridge                        7.8   \n",
      "2  3              Uptown                        3.8   \n",
      "3  4      Lincoln Square                        3.4   \n",
      "4  5        North Center                        0.3   \n",
      "\n",
      "  percent_households_below_poverty percent_aged_16_unemployed  \\\n",
      "0                             23.6                        8.7   \n",
      "1                             17.2                        8.8   \n",
      "2                               24                        8.9   \n",
      "3                             10.9                        8.2   \n",
      "4                              7.5                        5.2   \n",
      "\n",
      "  percent_aged_25_without_high_school_diploma  \\\n",
      "0                                        18.2   \n",
      "1                                        20.8   \n",
      "2                                        11.8   \n",
      "3                                        13.4   \n",
      "4                                         4.5   \n",
      "\n",
      "  percent_aged_under_18_or_over_64 per_capita_income_ hardship_index  \n",
      "0                             27.5              23939             39  \n",
      "1                             38.5              23040             46  \n",
      "2                             22.2              35787             20  \n",
      "3                             25.5              37524             17  \n",
      "4                             26.2              57123              6  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOADING CHICAGO CENSUS/DEMOGRAPHIC DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n2. Fetching census data from Chicago Data Portal...\")\n",
    "\n",
    "# Chicago community areas demographic data\n",
    "census_url = \"https://data.cityofchicago.org/resource/kn9c-c2s2.json\"\n",
    "\n",
    "try:\n",
    "    # Get census data via API\n",
    "    response = requests.get(census_url, params={\"$limit\": 100}, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    census_data = response.json()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    census_df = pd.DataFrame(census_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    census_path = '../data/raw/chicago_census_community_areas.csv'\n",
    "    census_df.to_csv(census_path, index=False)\n",
    "    \n",
    "    print(f\"   ✓ Downloaded census data for {len(census_df)} community areas\")\n",
    "    print(f\"   ✓ Saved to: {census_path}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n   Sample census data:\")\n",
    "    print(census_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Error downloading census data: {e}\")\n",
    "    print(\"   → Will try alternative source...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bb5c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DOWNLOADING CHICAGO CENSUS/DEMOGRAPHIC DATA\n",
      "======================================================================\n",
      "\n",
      "2. Fetching census data from Chicago Data Portal...\n",
      "   ✓ Downloaded census data for 78 community areas\n",
      "   ✓ Saved to: ../data/raw/chicago_census_community_areas.csv\n",
      "\n",
      "   Sample census data:\n",
      "  ca community_area_name percent_of_housing_crowded  \\\n",
      "0  1         Rogers Park                        7.7   \n",
      "1  2          West Ridge                        7.8   \n",
      "2  3              Uptown                        3.8   \n",
      "3  4      Lincoln Square                        3.4   \n",
      "4  5        North Center                        0.3   \n",
      "\n",
      "  percent_households_below_poverty percent_aged_16_unemployed  \\\n",
      "0                             23.6                        8.7   \n",
      "1                             17.2                        8.8   \n",
      "2                               24                        8.9   \n",
      "3                             10.9                        8.2   \n",
      "4                              7.5                        5.2   \n",
      "\n",
      "  percent_aged_25_without_high_school_diploma  \\\n",
      "0                                        18.2   \n",
      "1                                        20.8   \n",
      "2                                        11.8   \n",
      "3                                        13.4   \n",
      "4                                         4.5   \n",
      "\n",
      "  percent_aged_under_18_or_over_64 per_capita_income_ hardship_index  \n",
      "0                             27.5              23939             39  \n",
      "1                             38.5              23040             46  \n",
      "2                             22.2              35787             20  \n",
      "3                             25.5              37524             17  \n",
      "4                             26.2              57123              6  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOADING CHICAGO CENSUS/DEMOGRAPHIC DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n2. Fetching census data from Chicago Data Portal...\")\n",
    "\n",
    "# Chicago community areas demographic data\n",
    "census_url = \"https://data.cityofchicago.org/resource/kn9c-c2s2.json\"\n",
    "\n",
    "try:\n",
    "    # Get census data via API\n",
    "    response = requests.get(census_url, params={\"$limit\": 100}, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    census_data = response.json()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    census_df = pd.DataFrame(census_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    census_path = '../data/raw/chicago_census_community_areas.csv'\n",
    "    census_df.to_csv(census_path, index=False)\n",
    "    \n",
    "    print(f\"   ✓ Downloaded census data for {len(census_df)} community areas\")\n",
    "    print(f\"   ✓ Saved to: {census_path}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n   Sample census data:\")\n",
    "    print(census_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Error downloading census data: {e}\")\n",
    "    print(\"   → Will try alternative source...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7045020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA CLEANING & QUALITY CHECKS\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Total Records: 8,349,460\n",
      "\n",
      "2. Duplicate Check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Distinct records: 8,349,460\n",
      "   Duplicate records: 0\n",
      "\n",
      "3. Cleaning Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Records after cleaning: 8,349,460\n",
      "   Records removed: 0 (0.00%)\n",
      "\n",
      "4. Coordinate Validity Check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Valid coordinates: 8,321,923 (99.67%)\n",
      "   Invalid coordinates: 27,537 (0.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Final Clean Dataset: 8,321,923 records\n",
      "   ✓ Dataset cached in memory\n",
      "\n",
      "======================================================================\n",
      "✓ DATA CLEANING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Sample of clean data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:=====================================================> (49 + 1) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+------------+-------------+----+------+\n",
      "|date_ts            |primary_type   |latitude    |longitude    |year|season|\n",
      "+-------------------+---------------+------------+-------------+----+------+\n",
      "|2025-11-08 21:27:00|BATTERY        |41.80562868 |-87.623802064|2025|fall  |\n",
      "|2025-11-08 14:59:00|THEFT          |41.931574051|-87.711516699|2025|fall  |\n",
      "|2025-11-08 14:30:00|THEFT          |41.881722193|-87.624356274|2025|fall  |\n",
      "|2025-11-08 13:45:00|CRIMINAL DAMAGE|41.750223119|-87.6738307  |2025|fall  |\n",
      "|2025-11-08 12:29:00|THEFT          |41.883475491|-87.627876969|2025|fall  |\n",
      "+-------------------+---------------+------------+-------------+----+------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Continue with the rest of cleaning\n",
    "print(\"=\"*70)\n",
    "print(\"DATA CLEANING & QUALITY CHECKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Total records\n",
    "total_records = df.count()\n",
    "print(f\"\\n1. Total Records: {total_records:,}\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "print(\"\\n2. Duplicate Check:\")\n",
    "distinct_records = df.distinct().count()\n",
    "duplicates = total_records - distinct_records\n",
    "print(f\"   Distinct records: {distinct_records:,}\")\n",
    "print(f\"   Duplicate records: {duplicates:,}\")\n",
    "\n",
    "# 3. Remove records with missing critical data (coordinates and date)\n",
    "print(\"\\n3. Cleaning Data...\")\n",
    "df_clean = df.filter(\n",
    "    col(\"latitude\").isNotNull() & \n",
    "    col(\"longitude\").isNotNull() &\n",
    "    col(\"date_ts\").isNotNull() &\n",
    "    col(\"primary_type\").isNotNull()\n",
    ")\n",
    "\n",
    "cleaned_count = df_clean.count()\n",
    "removed = total_records - cleaned_count\n",
    "print(f\"   Records after cleaning: {cleaned_count:,}\")\n",
    "print(f\"   Records removed: {removed:,} ({(removed/total_records)*100:.2f}%)\")\n",
    "\n",
    "# 4. Check coordinate validity (Chicago boundaries)\n",
    "print(\"\\n4. Coordinate Validity Check:\")\n",
    "valid_coords = df_clean.filter(\n",
    "    (col(\"latitude\") >= 41.6) & (col(\"latitude\") <= 42.1) &  # Chicago lat range\n",
    "    (col(\"longitude\") >= -87.9) & (col(\"longitude\") <= -87.5)  # Chicago lon range\n",
    ").count()\n",
    "\n",
    "invalid_coords = cleaned_count - valid_coords\n",
    "print(f\"   Valid coordinates: {valid_coords:,} ({(valid_coords/cleaned_count)*100:.2f}%)\")\n",
    "print(f\"   Invalid coordinates: {invalid_coords:,} ({(invalid_coords/cleaned_count)*100:.2f}%)\")\n",
    "\n",
    "# 5. Filter to valid coordinates only\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"latitude\") >= 41.6) & (col(\"latitude\") <= 42.1) &\n",
    "    (col(\"longitude\") >= -87.9) & (col(\"longitude\") <= -87.5)\n",
    ")\n",
    "\n",
    "final_count = df_clean.count()\n",
    "print(f\"\\n5. Final Clean Dataset: {final_count:,} records\")\n",
    "\n",
    "# 6. Cache the clean dataset for faster processing\n",
    "df_clean.cache()\n",
    "print(f\"   ✓ Dataset cached in memory\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ DATA CLEANING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show sample of clean data\n",
    "print(\"\\nSample of clean data:\")\n",
    "df_clean.select(\"date_ts\", \"primary_type\", \"latitude\", \"longitude\", \"year\", \"season\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccf4f0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "# Use try_to_timestamp to handle invalid strings gracefully\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"date\",\n",
    "    expr(\"try_to_timestamp(date, 'MM/dd/yyyy hh:mm:ss a')\")\n",
    ")\n",
    "\n",
    "# Convert to DateType (optional)\n",
    "from pyspark.sql.functions import to_date\n",
    "df_clean = df_clean.withColumn(\"date_form\", to_date(col(\"date\")))\n",
    "\n",
    "# Drop the temporary timestamp column if you want\n",
    "# df_clean = df_clean.drop(\"date_ts\")\n",
    "\n",
    "# Save\n",
    "df_clean.write.mode(\"overwrite\").parquet(\"../data/crime_clean.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e29a50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark restarted with 8GB memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 22:22:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Restart Spark with MUCH more memory\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CrimePrediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✓ Spark restarted with 8GB memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9fe3dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATA (WITH SAMPLING FOR MEMORY EFFICIENCY)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:=================================>                     (11 + 7) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 2,000,000 records\n",
      "✓ Feature engineering complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load DATA WITH SAMPLING from the start\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA (WITH SAMPLING FOR MEMORY EFFICIENCY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load ONLY 2 million records to avoid memory issues\n",
    "df = spark.read.csv(\n",
    "    \"../data/raw/chicago_crimes_2001_present.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ").limit(2000000)  # Load only 2M records\n",
    "\n",
    "print(f\"✓ Loaded {df.count():,} records\")\n",
    "\n",
    "# Convert date\n",
    "df = df.withColumn(\"date_ts\", F.to_timestamp(\"Date\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "# Add features\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"year\", F.year(\"date_ts\"))\n",
    "    .withColumn(\"month\", F.month(\"date_ts\"))\n",
    "    .withColumn(\"hour\", F.hour(\"date_ts\"))\n",
    "    .withColumn(\"dayofweek\", F.date_format(\"date_ts\", \"E\"))\n",
    "    .withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.col(\"month\").isin(12, 1, 2), \"winter\")\n",
    "         .when(F.col(\"month\").isin(3, 4, 5), \"spring\")\n",
    "         .when(F.col(\"month\").isin(6, 7, 8), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    )\n",
    "    .withColumn(\"lat_round\", F.round(\"latitude\", 3))\n",
    "    .withColumn(\"lon_round\", F.round(\"longitude\", 3))\n",
    ")\n",
    "\n",
    "print(\"✓ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d687fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for crime data file...\n",
      "✓ Found crime data: ../data/raw/chicago_crimes_2001_present.csv\n",
      "\n",
      "File size: 2.19 GB\n"
     ]
    }
   ],
   "source": [
    "# Find the crime data file\n",
    "import os\n",
    "\n",
    "print(\"Looking for crime data file...\")\n",
    "\n",
    "# Check different possible locations\n",
    "possible_paths = [\n",
    "    \"../data/raw/\",\n",
    "    \"./data/raw/\",\n",
    "    \"../data/\",\n",
    "    \"./\",\n",
    "]\n",
    "\n",
    "crime_file = None\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        files = [f for f in os.listdir(path) if 'crime' in f.lower() or 'Crimes' in f]\n",
    "        if files:\n",
    "            crime_file = os.path.join(path, files[0])\n",
    "            print(f\"✓ Found crime data: {crime_file}\")\n",
    "            break\n",
    "\n",
    "if not crime_file:\n",
    "    print(\"✗ Crime data not found. Please provide the path to your crime CSV file.\")\n",
    "else:\n",
    "    print(f\"\\nFile size: {os.path.getsize(crime_file) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d0ca65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING CRIME DATA\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 2,000,000 crime records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:====================================>                  (12 + 6) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Clean records: 1,970,206\n",
      "✓ Feature engineering complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOADING CRIME DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Replace with your actual path from Step 1\n",
    "crime_path = crime_file  # Use the path found above\n",
    "\n",
    "# Load with limit for memory efficiency\n",
    "df = spark.read.csv(\n",
    "    crime_path,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").limit(2000000)  # 2M records\n",
    "\n",
    "print(f\"✓ Loaded {df.count():,} crime records\")\n",
    "\n",
    "# Convert date and add features\n",
    "df = df.withColumn(\"date_ts\", F.to_timestamp(\"Date\", \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"year\", F.year(\"date_ts\"))\n",
    "    .withColumn(\"month\", F.month(\"date_ts\"))\n",
    "    .withColumn(\"day\", F.dayofmonth(\"date_ts\"))\n",
    "    .withColumn(\"hour\", F.hour(\"date_ts\"))\n",
    "    .withColumn(\"dayofweek\", F.date_format(\"date_ts\", \"E\"))\n",
    "    .withColumn(\n",
    "        \"season\",\n",
    "        F.when(F.col(\"month\").isin(12, 1, 2), \"winter\")\n",
    "         .when(F.col(\"month\").isin(3, 4, 5), \"spring\")\n",
    "         .when(F.col(\"month\").isin(6, 7, 8), \"summer\")\n",
    "         .otherwise(\"fall\")\n",
    "    )\n",
    "    .withColumn(\"crime_date\", F.to_date(\"date_ts\"))  # Important for joining with weather\n",
    ")\n",
    "\n",
    "# Clean data\n",
    "df_clean = df.filter(\n",
    "    (F.col(\"latitude\").isNotNull()) & \n",
    "    (F.col(\"longitude\").isNotNull()) &\n",
    "    (F.col(\"date_ts\").isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"✓ Clean records: {df_clean.count():,}\")\n",
    "print(\"✓ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdcb7fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INTEGRATING WEATHER DATA\n",
      "======================================================================\n",
      "✓ Loaded 8,766 days of weather data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Integrated crime + weather: 1,970,206 records\n",
      "\n",
      "Sample of integrated data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:=================================>                     (11 + 7) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+-------------+----------+\n",
      "|date_ts            |Primary Type           |temp_mean|precipitation|wind_speed|\n",
      "+-------------------+-----------------------+---------+-------------+----------+\n",
      "|2025-11-09 00:00:00|OTHER OFFENSE          |NULL     |NULL         |NULL      |\n",
      "|2025-11-09 00:00:00|CRIMINAL SEXUAL ASSAULT|NULL     |NULL         |NULL      |\n",
      "|2025-11-09 00:00:00|DECEPTIVE PRACTICE     |NULL     |NULL         |NULL      |\n",
      "|2025-11-09 00:00:00|CRIMINAL SEXUAL ASSAULT|NULL     |NULL         |NULL      |\n",
      "|2025-11-09 00:00:00|SEX OFFENSE            |NULL     |NULL         |NULL      |\n",
      "+-------------------+-----------------------+---------+-------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTEGRATING WEATHER DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load weather data\n",
    "weather_path = \"../data/raw/chicago_weather_2001_2024.csv\"\n",
    "weather_spark = spark.read.csv(weather_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert weather date to date type\n",
    "weather_spark = weather_spark.withColumn(\"weather_date\", F.to_date(\"date\"))\n",
    "\n",
    "print(f\"✓ Loaded {weather_spark.count():,} days of weather data\")\n",
    "\n",
    "# Join crime data with weather data\n",
    "df_integrated = df_clean.join(\n",
    "    weather_spark,\n",
    "    df_clean.crime_date == weather_spark.weather_date,\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Integrated crime + weather: {df_integrated.count():,} records\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample of integrated data:\")\n",
    "df_integrated.select(\n",
    "    \"date_ts\", \"Primary Type\", \"temp_mean\", \"precipitation\", \"wind_speed\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7319ff79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of integrated data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 142:=================================>                     (11 + 7) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+-------------+----------+\n",
      "|date_ts            |Primary Type           |temp_mean|precipitation|wind_speed|\n",
      "+-------------------+-----------------------+---------+-------------+----------+\n",
      "|2025-11-09 00:00:00|OTHER OFFENSE          |NULL     |NULL         |NULL      |\n",
      "|2025-11-09 00:00:00|CRIMINAL SEXUAL ASSAULT|NULL     |NULL         |NULL      |\n",
      "|2025-11-09 00:00:00|DECEPTIVE PRACTICE     |NULL     |NULL         |NULL      |\n",
      "|2025-11-09 00:00:00|CRIMINAL SEXUAL ASSAULT|NULL     |NULL         |NULL      |\n",
      "|2025-11-09 00:00:00|SEX OFFENSE            |NULL     |NULL         |NULL      |\n",
      "+-------------------+-----------------------+---------+-------------+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show sample with correct column names (use backticks for columns with spaces)\n",
    "print(\"\\nSample of integrated data:\")\n",
    "df_integrated.select(\n",
    "    \"date_ts\", \n",
    "    F.col(\"Primary Type\"),  # Correct column name\n",
    "    \"temp_mean\", \n",
    "    \"precipitation\", \n",
    "    \"wind_speed\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e457982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INTEGRATING CENSUS/DEMOGRAPHIC DATA\n",
      "======================================================================\n",
      "✓ Loaded census data: 78 community areas\n",
      "\n",
      "Census columns available:\n",
      "root\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- community_area_name: string (nullable = true)\n",
      " |-- percent_of_housing_crowded: double (nullable = true)\n",
      " |-- percent_households_below_poverty: double (nullable = true)\n",
      " |-- percent_aged_16_unemployed: double (nullable = true)\n",
      " |-- percent_aged_25_without_high_school_diploma: double (nullable = true)\n",
      " |-- percent_aged_under_18_or_over_64: double (nullable = true)\n",
      " |-- per_capita_income_: integer (nullable = true)\n",
      " |-- hardship_index: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Final integrated dataset: 1,970,206 records\n",
      "✓ Final dataset cached\n",
      "\n",
      "======================================================================\n",
      "✓ ALL DATA INTEGRATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Sample of fully integrated data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 159:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+-------------+--------------+------------------+\n",
      "|date_ts            |Primary Type           |temp_mean|precipitation|Community Area|per_capita_income_|\n",
      "+-------------------+-----------------------+---------+-------------+--------------+------------------+\n",
      "|2025-11-09 00:00:00|OTHER OFFENSE          |NULL     |NULL         |43            |19398             |\n",
      "|2025-11-09 00:00:00|CRIMINAL SEXUAL ASSAULT|NULL     |NULL         |6             |60058             |\n",
      "|2025-11-09 00:00:00|DECEPTIVE PRACTICE     |NULL     |NULL         |24            |43198             |\n",
      "|2025-11-09 00:00:00|CRIMINAL SEXUAL ASSAULT|NULL     |NULL         |28            |44689             |\n",
      "|2025-11-09 00:00:00|SEX OFFENSE            |NULL     |NULL         |29            |12034             |\n",
      "|2025-11-09 00:00:00|CRIMINAL DAMAGE        |NULL     |NULL         |43            |19398             |\n",
      "|2025-11-09 00:00:00|DECEPTIVE PRACTICE     |NULL     |NULL         |8             |88669             |\n",
      "|2025-11-09 00:00:00|THEFT                  |NULL     |NULL         |28            |44689             |\n",
      "|2025-11-09 00:00:00|CRIMINAL SEXUAL ASSAULT|NULL     |NULL         |8             |88669             |\n",
      "|2025-11-09 00:00:00|BATTERY                |NULL     |NULL         |7             |71551             |\n",
      "+-------------------+-----------------------+---------+-------------+--------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTEGRATING CENSUS/DEMOGRAPHIC DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load census data - USE THE CORRECT FILENAME\n",
    "census_path = \"../data/raw/chicago_census_community_areas.csv\"  # ← FIXED PATH\n",
    "census_spark = spark.read.csv(census_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"✓ Loaded census data: {census_spark.count()} community areas\")\n",
    "\n",
    "# Show census columns to see what's available\n",
    "print(\"\\nCensus columns available:\")\n",
    "census_spark.printSchema()\n",
    "\n",
    "# The census data has \"ca\" column for community area number\n",
    "# Join with crime data by Community Area\n",
    "df_final = df_integrated.join(\n",
    "    census_spark,\n",
    "    df_integrated[\"Community Area\"] == census_spark[\"ca\"],  # ← Match column names\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Final integrated dataset: {df_final.count():,} records\")\n",
    "\n",
    "# Cache the final dataset\n",
    "df_final.cache()\n",
    "print(\"✓ Final dataset cached\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL DATA INTEGRATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show sample with all data sources - CORRECT COLUMN NAMES\n",
    "print(\"\\nSample of fully integrated data:\")\n",
    "df_final.select(\n",
    "    \"date_ts\", \n",
    "    F.col(\"Primary Type\"),\n",
    "    \"temp_mean\", \n",
    "    \"precipitation\", \n",
    "    F.col(\"Community Area\"),\n",
    "    F.col(\"per_capita_income_\")  # ← Fixed: added underscore\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae0fc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WEEK 2 - FINAL STATISTICAL SUMMARY REPORT\n",
      "Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\n",
      "======================================================================\n",
      "\n",
      "✅ DATASET OVERVIEW:\n",
      "   Total integrated records: 1,970,206\n",
      "   Weather data: 8,766 days (2001-2024)\n",
      "   Census areas: 78 community areas\n",
      "   Date range: 2001-2024\n",
      "\n",
      "✅ DATA SOURCES INTEGRATED:\n",
      "   ✓ Crime data (Chicago Police Department)\n",
      "   ✓ Weather data (Open-Meteo API)\n",
      "   ✓ Census/demographic data (Chicago Data Portal)\n",
      "\n",
      "✅ FEATURES ENGINEERED:\n",
      "   Temporal: year, month, day, hour, day_of_week, season\n",
      "   Weather: temperature, precipitation, wind_speed\n",
      "   Demographic: per_capita_income, hardship_index, poverty rates\n",
      "   Spatial: latitude, longitude, community_area\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHTS & PATTERNS\n",
      "======================================================================\n",
      "\n",
      "1. TOP 10 CRIME TYPES:\n",
      "+-------------------+------+\n",
      "|Primary Type       |count |\n",
      "+-------------------+------+\n",
      "|THEFT              |443553|\n",
      "|BATTERY            |363039|\n",
      "|CRIMINAL DAMAGE    |220436|\n",
      "|ASSAULT            |170465|\n",
      "|DECEPTIVE PRACTICE |134304|\n",
      "|MOTOR VEHICLE THEFT|129713|\n",
      "|OTHER OFFENSE      |125530|\n",
      "|BURGLARY           |72117 |\n",
      "|ROBBERY            |71516 |\n",
      "|NARCOTICS          |63936 |\n",
      "+-------------------+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "2. CRIMES BY SEASON (with average weather):\n",
      "+------+-----------+----------+---------------+\n",
      "|season|crime_count|avg_temp_F|avg_precip_inch|\n",
      "+------+-----------+----------+---------------+\n",
      "|  fall|     535286|      54.2|           0.13|\n",
      "|summer|     527446|      71.4|           0.16|\n",
      "|spring|     468207|      47.7|           0.14|\n",
      "|winter|     439267|      29.4|           0.08|\n",
      "+------+-----------+----------+---------------+\n",
      "\n",
      "\n",
      "3. PEAK CRIME HOURS:\n",
      "+----+------+\n",
      "|hour| count|\n",
      "+----+------+\n",
      "|   0|123607|\n",
      "|  12|115442|\n",
      "|  15|107195|\n",
      "|  17|106960|\n",
      "|  18|106849|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "4. CRIMES BY DAY OF WEEK:\n",
      "+---------+------+\n",
      "|dayofweek| count|\n",
      "+---------+------+\n",
      "|      Fri|291254|\n",
      "|      Sat|287521|\n",
      "|      Mon|281239|\n",
      "|      Sun|279898|\n",
      "|      Wed|278354|\n",
      "|      Thu|276235|\n",
      "|      Tue|275705|\n",
      "+---------+------+\n",
      "\n",
      "\n",
      "5. WEATHER IMPACT ON CRIME:\n",
      "   High Temperature Days (>80°F) vs Low Temperature Days (<40°F):\n",
      "   High temp crimes: 32,408\n",
      "   Low temp crimes: 573,071\n",
      "   Ratio: 0.06x more crimes in hot weather\n",
      "\n",
      "6. CRIME BY INCOME LEVELS:\n",
      "+--------------------+------+\n",
      "|        income_level| count|\n",
      "+--------------------+------+\n",
      "|Medium Income ($1...|952446|\n",
      "| High Income (>$30k)|553120|\n",
      "|  Low Income (<$15k)|464640|\n",
      "+--------------------+------+\n",
      "\n",
      "\n",
      "======================================================================\n",
      "✅ WEEK 2 DELIVERABLES COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "📋 DELIVERABLES CHECKLIST:\n",
      "   ✅ Clean, integrated dataset created\n",
      "      - Crime data cleaned and validated\n",
      "      - Weather data integrated by date\n",
      "      - Census data integrated by community area\n",
      "   ✅ Feature engineering completed\n",
      "      - Temporal features (year, month, hour, season, etc.)\n",
      "      - Weather features (temperature, precipitation, wind)\n",
      "      - Demographic features (income, poverty, education)\n",
      "   ✅ Exploratory Data Analysis completed\n",
      "      - Crime patterns by time analyzed\n",
      "      - Weather impact assessed\n",
      "      - Socioeconomic correlations identified\n",
      "   ✅ Statistical summary report generated\n",
      "\n",
      "🎯 READY FOR WEEK 3: Geospatial Analysis & Hotspot Detection\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 2 - FINAL STATISTICAL SUMMARY REPORT\")\n",
    "print(\"Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n✅ DATASET OVERVIEW:\")\n",
    "print(f\"   Total integrated records: {df_final.count():,}\")\n",
    "print(f\"   Weather data: 8,766 days (2001-2024)\")\n",
    "print(f\"   Census areas: {census_spark.count()} community areas\")\n",
    "print(f\"   Date range: 2001-2024\")\n",
    "\n",
    "print(f\"\\n✅ DATA SOURCES INTEGRATED:\")\n",
    "print(f\"   ✓ Crime data (Chicago Police Department)\")\n",
    "print(f\"   ✓ Weather data (Open-Meteo API)\")\n",
    "print(f\"   ✓ Census/demographic data (Chicago Data Portal)\")\n",
    "\n",
    "print(f\"\\n✅ FEATURES ENGINEERED:\")\n",
    "print(f\"   Temporal: year, month, day, hour, day_of_week, season\")\n",
    "print(f\"   Weather: temperature, precipitation, wind_speed\")\n",
    "print(f\"   Demographic: per_capita_income, hardship_index, poverty rates\")\n",
    "print(f\"   Spatial: latitude, longitude, community_area\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS & PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Top 10 Crime Types\n",
    "print(\"\\n1. TOP 10 CRIME TYPES:\")\n",
    "df_final.groupBy(F.col(\"Primary Type\")).count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show(10, truncate=False)\n",
    "\n",
    "# 2. Crimes by Season with Weather\n",
    "print(\"\\n2. CRIMES BY SEASON (with average weather):\")\n",
    "df_final.groupBy(\"season\").agg(\n",
    "    F.count(\"*\").alias(\"crime_count\"),\n",
    "    F.round(F.avg(\"temp_mean\"), 1).alias(\"avg_temp_F\"),\n",
    "    F.round(F.avg(\"precipitation\"), 2).alias(\"avg_precip_inch\")\n",
    ").orderBy(F.desc(\"crime_count\")).show()\n",
    "\n",
    "# 3. Peak Crime Hours\n",
    "print(\"\\n3. PEAK CRIME HOURS:\")\n",
    "df_final.groupBy(\"hour\").count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show(5)\n",
    "\n",
    "# 4. Crime by Day of Week\n",
    "print(\"\\n4. CRIMES BY DAY OF WEEK:\")\n",
    "df_final.groupBy(\"dayofweek\").count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show()\n",
    "\n",
    "# 5. Weather Impact Analysis\n",
    "print(\"\\n5. WEATHER IMPACT ON CRIME:\")\n",
    "print(\"   High Temperature Days (>80°F) vs Low Temperature Days (<40°F):\")\n",
    "high_temp = df_final.filter(F.col(\"temp_mean\") > 80).count()\n",
    "low_temp = df_final.filter(F.col(\"temp_mean\") < 40).count()\n",
    "print(f\"   High temp crimes: {high_temp:,}\")\n",
    "print(f\"   Low temp crimes: {low_temp:,}\")\n",
    "print(f\"   Ratio: {high_temp/low_temp:.2f}x more crimes in hot weather\")\n",
    "\n",
    "# 6. Socioeconomic Analysis\n",
    "print(\"\\n6. CRIME BY INCOME LEVELS:\")\n",
    "df_final.groupBy(\n",
    "    F.when(F.col(\"per_capita_income_\") < 15000, \"Low Income (<$15k)\")\n",
    "     .when((F.col(\"per_capita_income_\") >= 15000) & (F.col(\"per_capita_income_\") < 30000), \"Medium Income ($15k-$30k)\")\n",
    "     .otherwise(\"High Income (>$30k)\")\n",
    "     .alias(\"income_level\")\n",
    ").count().orderBy(F.desc(\"count\")).show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ WEEK 2 DELIVERABLES COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n📋 DELIVERABLES CHECKLIST:\")\n",
    "print(\"   ✅ Clean, integrated dataset created\")\n",
    "print(\"      - Crime data cleaned and validated\")\n",
    "print(\"      - Weather data integrated by date\")\n",
    "print(\"      - Census data integrated by community area\")\n",
    "print(\"   ✅ Feature engineering completed\")\n",
    "print(\"      - Temporal features (year, month, hour, season, etc.)\")\n",
    "print(\"      - Weather features (temperature, precipitation, wind)\")\n",
    "print(\"      - Demographic features (income, poverty, education)\")\n",
    "print(\"   ✅ Exploratory Data Analysis completed\")\n",
    "print(\"      - Crime patterns by time analyzed\")\n",
    "print(\"      - Weather impact assessed\")\n",
    "print(\"      - Socioeconomic correlations identified\")\n",
    "print(\"   ✅ Statistical summary report generated\")\n",
    "\n",
    "print(\"\\n🎯 READY FOR WEEK 3: Geospatial Analysis & Hotspot Detection\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54340389",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[COLUMN_ALREADY_EXISTS] The column `date` already exists. Choose another name or rename the existing column. SQLSTATE: 42711",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/processed/integrated_crime_data.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Saved integrated dataset for Week 3!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyspark/sql/readwriter.py:2003\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2002\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [COLUMN_ALREADY_EXISTS] The column `date` already exists. Choose another name or rename the existing column. SQLSTATE: 42711"
     ]
    }
   ],
   "source": [
    "df_final.write.parquet(\"../data/processed/integrated_crime_data.parquet\", mode=\"overwrite\")\n",
    "print(\"✓ Saved integrated dataset for Week 3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b74a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING WEEK 2 DATA FOR WEEK 3\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved integrated dataset to: ../data/processed/integrated_crime_data.parquet\n",
      "✓ Records saved: 1,970,206\n",
      "\n",
      "✅ Week 2 data preserved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the integrated dataset from Week 2 - FIXED\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING WEEK 2 DATA FOR WEEK 3\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select only the columns we need (avoid duplicates)\n",
    "df_to_save = df_final.select(\n",
    "    \"date_ts\",\n",
    "    F.col(\"Primary Type\").alias(\"crime_type\"),\n",
    "    F.col(\"Latitude\").alias(\"latitude\"),\n",
    "    F.col(\"Longitude\").alias(\"longitude\"),\n",
    "    F.col(\"Community Area\").alias(\"community_area\"),\n",
    "    \"temp_mean\",\n",
    "    \"precipitation\",\n",
    "    \"wind_speed\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"season\",\n",
    "    F.col(\"per_capita_income_\").alias(\"per_capita_income\")\n",
    ")\n",
    "\n",
    "# Save to Parquet\n",
    "output_path = \"../data/processed/integrated_crime_data.parquet\"\n",
    "df_to_save.write.parquet(output_path, mode=\"overwrite\")\n",
    "\n",
    "print(f\"✓ Saved integrated dataset to: {output_path}\")\n",
    "print(f\"✓ Records saved: {df_to_save.count():,}\")\n",
    "print(\"\\n✅ Week 2 data preserved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "✓ All Week 3 libraries installed!\n"
     ]
    }
   ],
   "source": [
    "# Install libraries needed for Week 3\n",
    "!pip install h3 folium scikit-learn matplotlib seaborn geopandas\n",
    "!pip install pysal esda libpysal\n",
    "\n",
    "print(\"✓ All Week 3 libraries installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "WEEK 3: GEOSPATIAL ANALYSIS & HOTSPOT DETECTION\n",
      "Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\n",
      "======================================================================\n",
      "✓ Loaded 1,970,206 records\n",
      "\n",
      "Available columns:\n",
      "['date_ts', 'crime_type', 'latitude', 'longitude', 'community_area', 'temp_mean', 'precipitation', 'wind_speed', 'year', 'month', 'hour', 'dayofweek', 'season', 'per_capita_income']\n",
      "\n",
      "Converting to Pandas for geospatial analysis...\n",
      "✓ Converted 1,970,206 records to Pandas\n",
      "\n",
      "✅ Data ready for geospatial analysis!\n",
      "\n",
      "Sample data:\n",
      "     date_ts               crime_type   latitude  longitude  community_area  \\\n",
      "0 2025-11-09            OTHER OFFENSE  41.765286 -87.577086            43.0   \n",
      "1 2025-11-09  CRIMINAL SEXUAL ASSAULT  41.936336 -87.650710             6.0   \n",
      "2 2025-11-09       DECEPTIVE PRACTICE  41.904817 -87.689930            24.0   \n",
      "3 2025-11-09  CRIMINAL SEXUAL ASSAULT  41.877609 -87.667595            28.0   \n",
      "4 2025-11-09              SEX OFFENSE  41.860902 -87.707037            29.0   \n",
      "\n",
      "   temp_mean  year  month  hour season  \n",
      "0        NaN  2025     11     0   fall  \n",
      "1        NaN  2025     11     0   fall  \n",
      "2        NaN  2025     11     0   fall  \n",
      "3        NaN  2025     11     0   fall  \n",
      "4        NaN  2025     11     0   fall  \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"WEEK 3: GEOSPATIAL ANALYSIS & HOTSPOT DETECTION\")\n",
    "print(\"Team: Kiran Ghumare, Neethu Satravada, Sajitha Mathi\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the saved data\n",
    "df_geo = spark.read.parquet(\"../data/processed/integrated_crime_data.parquet\")\n",
    "\n",
    "print(f\"✓ Loaded {df_geo.count():,} records\")\n",
    "\n",
    "# Show column names to confirm\n",
    "print(\"\\nAvailable columns:\")\n",
    "print(df_geo.columns)\n",
    "\n",
    "# Convert to Pandas - use correct column names (already lowercase)\n",
    "print(\"\\nConverting to Pandas for geospatial analysis...\")\n",
    "df_pandas = df_geo.select(\n",
    "    \"date_ts\",\n",
    "    \"crime_type\",      # Already renamed\n",
    "    \"latitude\",        # Already renamed\n",
    "    \"longitude\",       # Already renamed\n",
    "    \"community_area\",  # Already renamed\n",
    "    \"temp_mean\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"hour\",\n",
    "    \"season\"\n",
    ").toPandas()\n",
    "\n",
    "print(f\"✓ Converted {len(df_pandas):,} records to Pandas\")\n",
    "print(\"\\n✅ Data ready for geospatial analysis!\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 22:13:40 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark session restarted with localhost binding.\n",
      "\n",
      "Converting to Pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Success! Converted 1,970,206 records to Pandas.\n"
     ]
    }
   ],
   "source": [
    "# 1. Stop the broken session\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import os\n",
    "# Force Python to use localhost\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 2. Restart Spark with specific network bindings\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Week3_Geospatial\")\n",
    "    # Network Fixes (Crucial for macOS)\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    # Memory Settings for 2M rows\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") # 0 = unlimited\n",
    "    # Optimization for Pandas conversion\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"✓ Spark session restarted with localhost binding.\")\n",
    "\n",
    "# 3. Reload the data\n",
    "df_geo = spark.read.parquet(\"../data/processed/integrated_crime_data.parquet\")\n",
    "\n",
    "# 4. Try the conversion again (using Arrow for speed)\n",
    "print(\"\\nConverting to Pandas...\")\n",
    "df_pandas = df_geo.select(\n",
    "    \"date_ts\", \"crime_type\", \"latitude\", \"longitude\", \n",
    "    \"community_area\", \"temp_mean\", \"year\", \"month\", \"hour\", \"season\"\n",
    ").toPandas()\n",
    "\n",
    "print(f\"✓ Success! Converted {len(df_pandas):,} records to Pandas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0000bfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h3 in /opt/homebrew/lib/python3.11/site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "======================================================================\n",
      "H3 HEXAGONAL SPATIAL INDEXING\n",
      "======================================================================\n",
      "h3 version: 4.3.1\n",
      "\n",
      "Generating H3 hexagonal grid indices...\n",
      "✓ Added H3 indices to 1,970,206 records\n",
      "✓ Created 893 hexagonal cells\n",
      "✓ Average crimes per hex: 2206.3\n",
      "\n",
      "Top 10 Crime Hotspot Hexagons:\n",
      "            h3_index  crime_count  lat_center  lon_center\n",
      "224  882664c1a9fffff        30767   41.881964  -87.628032\n",
      "234  882664c1e1fffff        26457   41.894715  -87.625989\n",
      "237  882664c1e7fffff        16714   41.889090  -87.631855\n",
      "235  882664c1e3fffff        14558   41.887668  -87.622796\n",
      "240  882664c1edfffff        10835   41.903049  -87.630709\n",
      "673  882664ceb5fffff        10554   41.755311  -87.560330\n",
      "303  882664c8cbfffff        10138   41.861043  -87.712294\n",
      "383  882664caa7fffff         9543   41.875766  -87.723370\n",
      "551  882664cce1fffff         9426   41.745956  -87.605168\n",
      "248  882664c811fffff         9403   41.877779  -87.745298\n",
      "\n",
      "✅ H3 spatial indexing complete!\n"
     ]
    }
   ],
   "source": [
    "%pip install h3 --break-system-packages\n",
    "\n",
    "\n",
    "import h3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"H3 HEXAGONAL SPATIAL INDEXING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check h3 version\n",
    "print(f\"h3 version: {h3.__version__}\")\n",
    "\n",
    "# Add H3 index to each crime (resolution 8 = ~0.46 km² hexagons)\n",
    "print(\"\\nGenerating H3 hexagonal grid indices...\")\n",
    "\n",
    "# Use the correct function based on h3 version\n",
    "def get_h3_index(lat, lon, resolution=8):\n",
    "    try:\n",
    "        # Try new API (h3 v4+)\n",
    "        return h3.latlng_to_cell(lat, lon, resolution)\n",
    "    except AttributeError:\n",
    "        # Fall back to old API (h3 v3)\n",
    "        return h3.geo_to_h3(lat, lon, resolution)\n",
    "\n",
    "df_pandas['h3_index'] = df_pandas.apply(\n",
    "    lambda row: get_h3_index(row['latitude'], row['longitude'], 8),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"✓ Added H3 indices to {len(df_pandas):,} records\")\n",
    "\n",
    "# Aggregate crimes by hexagon\n",
    "hex_crimes = df_pandas.groupby('h3_index').agg({\n",
    "    'crime_type': 'count',\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "hex_crimes.columns = ['h3_index', 'crime_count', 'lat_center', 'lon_center']\n",
    "\n",
    "print(f\"✓ Created {len(hex_crimes):,} hexagonal cells\")\n",
    "print(f\"✓ Average crimes per hex: {hex_crimes['crime_count'].mean():.1f}\")\n",
    "\n",
    "# Show top crime hotspot hexagons\n",
    "print(\"\\nTop 10 Crime Hotspot Hexagons:\")\n",
    "print(hex_crimes.nlargest(10, 'crime_count')[['h3_index', 'crime_count', 'lat_center', 'lon_center']])\n",
    "\n",
    "print(\"\\n✅ H3 spatial indexing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h3 in /opt/homebrew/lib/python3.11/site-packages (4.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U --pre h3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
